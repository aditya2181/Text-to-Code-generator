{"cells":[{"cell_type":"code","execution_count":null,"id":"f2ce55e5","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2022-04-21T14:41:08.079346Z","iopub.status.busy":"2022-04-21T14:41:08.078576Z","iopub.status.idle":"2022-04-21T14:41:08.757995Z","shell.execute_reply":"2022-04-21T14:41:08.757405Z","shell.execute_reply.started":"2022-04-21T12:12:57.975662Z"},"papermill":{"duration":0.706816,"end_time":"2022-04-21T14:41:08.758209","exception":false,"start_time":"2022-04-21T14:41:08.051393","status":"completed"},"tags":[],"id":"f2ce55e5"},"outputs":[],"source":["!cp \"tf-custom-training-loop-figure/tf_process.png\" \"../working/\""]},{"cell_type":"code","execution_count":null,"id":"2a495960","metadata":{"execution":{"iopub.execute_input":"2022-04-21T14:41:08.802692Z","iopub.status.busy":"2022-04-21T14:41:08.802136Z","iopub.status.idle":"2022-04-21T14:41:34.124951Z","shell.execute_reply":"2022-04-21T14:41:34.124136Z","shell.execute_reply.started":"2022-04-21T12:12:58.731052Z"},"papermill":{"duration":25.348195,"end_time":"2022-04-21T14:41:34.125085","exception":false,"start_time":"2022-04-21T14:41:08.776890","status":"completed"},"tags":[],"id":"2a495960","outputId":"1ece5eb7-e717-4fce-f495-62f091fe9252"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n","TF version 2.6.2\n","Num GPUs Available:  1\n"]}],"source":["import os\n","import time\n","import math\n","import random\n","import datetime\n","from pathlib import Path\n","\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # reduce the amount of console output from TF\n","import tensorflow as tf\n","\n","from transformers import *\n","!pip install -q datasets # install HF datasets library\n","from datasets import load_dataset\n","\n","logging.set_verbosity_warning()\n","logging.set_verbosity_error()\n","\n","import logging\n","\n","print('TF version',tf.__version__)\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) # check GPU available"]},{"cell_type":"markdown","id":"9bac603b","metadata":{"execution":{"iopub.execute_input":"2022-04-17T07:17:48.466256Z","iopub.status.busy":"2022-04-17T07:17:48.465961Z","iopub.status.idle":"2022-04-17T07:17:48.476242Z","shell.execute_reply":"2022-04-17T07:17:48.475399Z","shell.execute_reply.started":"2022-04-17T07:17:48.466215Z"},"papermill":{"duration":0.019523,"end_time":"2022-04-21T14:41:34.165174","exception":false,"start_time":"2022-04-21T14:41:34.145651","status":"completed"},"tags":[],"id":"9bac603b"},"source":["<a id=\"section5\"><font color='#FF6F00'><h2>Setup Strategy</h2></font></a>\n","\n","<a id=\"section5a\"><font color=#4583><h3>Mixed Precision Training</h3></font></a>\n","\n","- Floating point types store numeric information of three types – the sign, exponent, and fraction.  Traditional float32 representations have 8 bits and 23 bits respectively to represent the exponent and fraction.  Traditional float16 representations (the format used for NVIDIA hardware) roughly halve both the exponent and fraction components of the representation. TPUs use a variant called bfloat16.\n","\n","- Most of a transformer network can be naively converted to float16 weights and activations with no accuracy penalty.\n","\n","<br>\n","<center><img src=\"https://www.pragmatic.ml/content/images/2020/04/image-2.png\" alt=\"fp16\" width=\"600\"/></center>\n","<br>\n","\n","- Small portions of the network – in particular, portions of the softmax operation – must remain in float32.  This is because the sum of a large number of small values (our logits) can be a source of accumulated error. Because both float16 and float32 values are used, this method is often referred to as \"mixed-precision\" training.\n","  \n","- Less precise numeric representations enable speedups from two sources.\n"," - Native half-precision instructions\n"," - Larger batch sizes thanks to more compact representations\n","\n","- Note:\n","   > While mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs and Cloud TPUs. NVIDIA GPUs support using a mix of float16 and float32, while TPUs support a mix of bfloat16 and float32. Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. - TensorFlow\n","   \n","<a id=\"section5b\"><font color=#4583><h3>XLA: Optimizing Compiler for Machine Learning</h3></font></a>\n","\n","- XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n","\n","<br>\n","<center><img src=\"https://www.tensorflow.org/xla/images/tf_xla_performance.png\" alt=\"xla\" width=\"500\" height=\"500\"/></center>\n","<br>\n","\n","- When a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU kernel implementation that the executor dispatches to. XLA provides an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization. For example, let's look at an optimization XLA does in the context of a simple TensorFlow computation:\n","\n","    ```\n","    def model_fn(x, y, z):\n","      return tf.reduce_sum(x + y * z)\n","    ```\n","\n","- XLA can optimize the graph so that it computes the result in a single kernel launch. It does this by \"fusing\" the addition, multiplication and reduction into a single GPU kernel. Moreover, this fused operation does not write out the intermediate values produced by y*z and x+y*z to memory; instead it \"streams\" the results of these intermediate computations directly to their users while keeping them entirely in GPU registers. Fusion is XLA's single most important optimization.\n","\n","<a id=\"section5c\"><font color=#4583><h3>Distribution Strategy</h3></font></a>\n","- `tf.distribute.Strategy` is a TensorFlow API to distribute training across multiple GPUs, multiple machines, or TPUs. Using this API, we can distribute our existing models and training code with minimal code changes.\n","\n","- `tf.distribute.Strategy` intends to cover a number of use cases along different axes. Some of these axes are:\n","\n","    - Synchronous vs asynchronous training: These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n","    - Hardware platform: You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\n","    \n","- We can use `tf.distribute.Strategy` with very few changes to our code, because the underlying components of TensorFlow have been changed to become strategy-aware. This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints.\n","\n","- We can execute our programs eagerly, or in a graph using `tf.function`. It intends to support both these modes of execution, but works best with graph execution.\n","\n","- The `tf.distribute.Strategy` classes provide a core set of methods to support custom training loops. Using these may require minor restructuring of the code initially, but once that is done, we should be able to switch between GPUs, TPUs, and multiple machines simply by changing the strategy instance.\n","\n","- *Note*: For single GPU we have `tf.distribute.OneDeviceStrategy`. This is a strategy to place all variables and computation on a single specified device.\n"," > This strategy is distinct from the Default Strategy in a number of ways. In the Default Strategy, the variable placement logic remains unchanged when compared to running TensorFlow without any distribution strategy. But when using OneDeviceStrategy, all variables created in its scope are explicitly placed on the specified device. Moreover, any functions called via OneDeviceStrategy.run will also be placed on the specified device. Input distributed through this strategy will be prefetched to the specified device. In the Default Strategy, there is no input distribution."]},{"cell_type":"code","execution_count":null,"id":"bab0a377","metadata":{"execution":{"iopub.execute_input":"2022-04-21T14:41:34.216338Z","iopub.status.busy":"2022-04-21T14:41:34.215392Z","iopub.status.idle":"2022-04-21T14:41:34.218572Z","shell.execute_reply":"2022-04-21T14:41:34.218991Z","shell.execute_reply.started":"2022-04-21T12:13:24.841197Z"},"papermill":{"duration":0.03458,"end_time":"2022-04-21T14:41:34.219150","exception":false,"start_time":"2022-04-21T14:41:34.184570","status":"completed"},"tags":[],"id":"bab0a377","outputId":"0276b277-0cea-4dc6-b888-518912df1969"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Tensorflow: setting up strategy\n"," XLA Enabled\n"," One Device Strategy [GPU] Enabled\n"]}],"source":["def setup_strategy(xla, fp16, no_cuda):\n","    print(\" Tensorflow: setting up strategy\")\n","\n","    # setup xla\n","    if xla:\n","        print(\" XLA Enabled\")\n","        tf.config.optimizer.set_jit(True)\n","\n","    # setup mixed precision training\n","    if fp16:\n","        # Set to float16 at first\n","        print(\" Mixed Precision Training Enabled\")\n","        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n","        tf.keras.mixed_precision.experimental.set_policy(policy)\n","\n","    # setup distribution strategy\n","    gpus = tf.config.list_physical_devices(\"GPU\")\n","    if no_cuda:\n","        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n","    else:\n","        if len(gpus) == 0:\n","            print(\" One Device Strategy [CPU] Enabled\")\n","            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n","        elif len(gpus) == 1:\n","            print(\" One Device Strategy [GPU] Enabled\")\n","            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n","        elif len(gpus) > 1:\n","            print(\" Mirrored Strategy Enabled\")\n","            # If only want to use a specific subset of GPUs use CUDA_VISIBLE_DEVICES=0`\n","            strategy = tf.distribute.MirroredStrategy()\n","        else:\n","            strategy = tf.distribute.get_strategy()\n","\n","    return strategy\n","\n","def n_replicas(strategy):\n","    # return number of devices\n","    return strategy.num_replicas_in_sync\n","\n","# note:\n","# huggingface TF-T5 implementation has issues when mixed precision is enabled\n","# we will disable FP16 for this but can be used for training any other model\n","strategy = setup_strategy(xla=True, fp16=False, no_cuda=False)"]},{"cell_type":"markdown","id":"1bcb2f54","metadata":{"papermill":{"duration":0.018996,"end_time":"2022-04-21T14:41:34.257755","exception":false,"start_time":"2022-04-21T14:41:34.238759","status":"completed"},"tags":[],"id":"1bcb2f54"},"source":["<a id=\"section6\"><font color='#FF6F00'><h2>MBPP (Mostly Basic Python Problems) Benchmark</h2></font></a>\n","\n","- The Mostly Basic Programming Problems dataset was introduced in [Program Synthesis with Large Language Models](https://arxiv.org/pdf/2108.07732.pdf). It contains 974 short Python functions designed to be solved by entry-level programmers, text descriptions of those programs, and test cases to check for functional correctness. This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand verified by the authors.\n","\n","\n","\n","- The problems range from simple numeric manipulations or tasks that require basic usage of standard library functions to tasks that require nontrivial external knowledge, such as the definition of particular notable integer sequences.\n","\n","- Given this, they manually inspected, edited, and pruned a subset of the questions, yielding 426 hand-verified questions, which we refer to as the edited dataset. For each question in the edited dataset, they ensured it had a standard Python function signature, that it was unambiguous to a human, and that its test cases accurately reflected the text description.\n","\n","- The original and cleaned dataset is available [here](https://github.com/google-research/google-research/tree/master/mbpp).\n","\n","<a id=\"section7\"><font color='#FF6F00'><h2>Dataset Processing</h2></font></a>\n","\n","Below is the complete process from data downloading to processing and converting to TensorFlow `tf.data.Dataset` object. It gives an overview of our data loading pipeline but to have a comprehensive idea one can read about tf.Data API [here](https://www.tensorflow.org/guide/data) and how to optimize data pipeline performance [here](https://www.tensorflow.org/guide/data_performance)\n","\n","*Note: check `run()` method for complete code flow.*\n","\n","- `download_dataset()` downloads the mbpp dataset from url and we load using hf datasets `load_dataset()` function.\n","- `convert_examples_to_features()` creates features (input_ids, attention_mask, labels) required for our model training using `map()` function.\n","- split the training data into train/test with test_size of size 0.1% of total training data using `split()` method.\n","- `get_train_tfdataset()` and `get_validation_tfdataset()`converts the dataset to tensorflow tf.data.Dataset object.\n","\n","    - The `Dataset.from_generator` constructor converts the python generator to a fully functional `tf.data.Dataset`. The constructor takes a callable as input, not an iterator. This allows it to restart the generator when it reaches the end. The `output_types` argument is required because `tf.data` builds a `tf.Graph` internally, and graph edges require a `tf.dtype`. It's also important to note that the `output_shapes` and `output_types` follow the same nesting rules as other dataset methods.\n","    \n","    - The simplest way to iterate over a dataset in multiple epochs is to use the `Dataset.repeat()` transformation. Applying the `Dataset.repeat()` transformation with no arguments will repeat the input indefinitely.\n","    - The `Dataset.shuffle()` transformation maintains a fixed-size buffer and chooses the next element uniformly at random from that buffer. We apply `Dataset.shuffle()` only for training dataset and not for validation dataset.\n","    - A `Dataset.batch` applied after `Dataset.repeat` will yield batches that straddle epoch boundaries. If you need clear epoch separation, put `Dataset.batch` before the repeat.\n","    - Prefetching overlaps the preprocessing and model execution of a training step. While the model is executing training step `s`, the input pipeline is reading the data for step `s+1`. Doing so reduces the step time to the maximum (as opposed to the sum) of the training and the time it takes to extract the data. The `tf.data` API provides the `tf.data.Dataset.prefetch` transformation. It can be used to decouple the time when data is produced from the time when data is consumed. The number of elements to prefetch should be equal to (or possibly greater than) the number of batches consumed by a single training step. You could either manually tune this value, or set it to `tf.data.AUTOTUNE`, which will prompt the `tf.data` runtime to tune the value dynamically at runtime."]},{"cell_type":"code","execution_count":null,"id":"70606042","metadata":{"execution":{"iopub.execute_input":"2022-04-21T14:41:34.316411Z","iopub.status.busy":"2022-04-21T14:41:34.315493Z","iopub.status.idle":"2022-04-21T14:41:34.318389Z","shell.execute_reply":"2022-04-21T14:41:34.317799Z","shell.execute_reply.started":"2022-04-21T12:13:24.856404Z"},"papermill":{"duration":0.040268,"end_time":"2022-04-21T14:41:34.318549","exception":false,"start_time":"2022-04-21T14:41:34.278281","status":"completed"},"tags":[],"id":"70606042"},"outputs":[],"source":["def download_dataset(cache_dir):\n","    # download data using a keras utility\n","    _url = \"https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl\" # download mbpp dataset\n","    dataset_path = tf.keras.utils.get_file(\"mbpp.jsonl\", origin=_url, cache_dir=cache_dir, cache_subdir=cache_dir)\n","    return dataset_path\n","\n","def convert_examples_to_features(examples, tokenizer, args):\n","    # encode text-code pairs\n","    texts = examples['text']\n","    codes = examples['code']\n","    # tests = [\" \".join(test) for test in examples['test_list']] # convert list of test cases to single string\n","\n","    # encode texts by prepending the task for input sequence\n","    inputs = [args.prefix + text for text in texts]\n","    model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n","\n","    # encode texts by prepending the task for input sequence and appending the test sequence\n","    # inputs = [args.prefix + text + \" \" + test for text, test in zip(texts, tests)]\n","    # model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n","\n","    # encode texts by prepending the task for input sequence\n","    labels = tokenizer(codes, max_length=args.max_target_length, padding=\"max_length\", truncation=True).input_ids\n","\n","    # we need to replace the index of the padding tokens by -100\n","    # such that they are not taken into account by the CrossEntropyLoss\n","    labels_with_ignore_index = []\n","    for labels_example in labels:\n","        labels_example = [label if label != 0 else -100 for label in labels_example]\n","        labels_with_ignore_index.append(labels_example)\n","    model_inputs[\"labels\"] = labels_with_ignore_index\n","\n","    # return features\n","    return model_inputs\n","\n","\n","def get_train_tfdataset(train_dataset, num_train_examples, args):\n","    # select feature columns\n","    columns = ['input_ids', 'attention_mask', 'labels']\n","    # set to tensorflow format\n","    train_dataset.set_format(type='tensorflow', columns=columns)\n","\n","    # specify return types\n","    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n","    # specify return shapes\n","    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n","    # initialize dataset\n","    tf_dataset = tf.data.Dataset.from_generator(lambda : train_dataset, return_types, return_shapes)\n","\n","    # turn off auto-sharding\n","    options = tf.data.Options()\n","    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n","    tf_dataset = tf_dataset.with_options(options)\n","\n","    # repeat, shuffle, batch, prefetch\n","    ds = (\n","        tf_dataset.repeat()\n","        .shuffle(num_train_examples, seed=args.seed)\n","        .batch(args.train_batch_size)\n","        .prefetch(tf.data.AUTOTUNE)\n","    )\n","\n","    # distribute dataset to devices\n","    return strategy.experimental_distribute_dataset(ds)\n","\n","def get_validation_tfdataset(eval_dataset, num_validation_examples, args):\n","    # select feature columns\n","    columns = ['input_ids', 'attention_mask', 'labels']\n","    # set to tensorflow format\n","    eval_dataset.set_format(type='tensorflow', columns=columns)\n","\n","    # specify return types\n","    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n","    # specify return shapes\n","    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n","    # initialize dataset\n","    tf_dataset = tf.data.Dataset.from_generator(lambda : eval_dataset, return_types, return_shapes)\n","\n","    # turn off auto-sharding\n","    options = tf.data.Options()\n","    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n","    tf_dataset = tf_dataset.with_options(options)\n","\n","    # repeat, batch, prefetch\n","    ds = (\n","        tf_dataset.repeat()\n","        .batch(args.validation_batch_size)\n","        .prefetch(tf.data.AUTOTUNE)\n","    )\n","\n","    # distribute dataset to devices\n","    return strategy.experimental_distribute_dataset(ds)"]},{"cell_type":"markdown","id":"29a2385d","metadata":{"papermill":{"duration":0.019497,"end_time":"2022-04-21T14:41:34.358404","exception":false,"start_time":"2022-04-21T14:41:34.338907","status":"completed"},"tags":[],"id":"29a2385d"},"source":["<a id=\"section8\"><font color='#FF6F00'><h2>Utility Functions / Class</h2></font></a>\n","\n","- *fix_all_seeds()* - sets the random seed for deterministic results.\n","- *init_logger()* - initialize logger for tracking events.\n","- *ProgressBar()* - custom progress bar to display metrics."]},{"cell_type":"code","execution_count":null,"id":"d8e03f5d","metadata":{"execution":{"iopub.execute_input":"2022-04-21T14:41:34.415759Z","iopub.status.busy":"2022-04-21T14:41:34.414688Z","iopub.status.idle":"2022-04-21T14:41:34.416494Z","shell.execute_reply":"2022-04-21T14:41:34.416930Z","shell.execute_reply.started":"2022-04-21T12:13:24.876208Z"},"papermill":{"duration":0.039653,"end_time":"2022-04-21T14:41:34.417075","exception":false,"start_time":"2022-04-21T14:41:34.377422","status":"completed"},"tags":[],"id":"d8e03f5d"},"outputs":[],"source":["def fix_all_seeds(seed):\n","    # set random seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","def init_logger(log_file=None, log_file_level=logging.NOTSET):\n","    # initialize logger for tracking events and save in file\n","    if isinstance(log_file, Path):\n","        log_file = str(log_file)\n","    log_format = logging.Formatter(\n","        fmt='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n","        datefmt='%m/%d/%Y %H:%M:%S'\n","    )\n","    logger = logging.getLogger()\n","    logger.setLevel(logging.INFO)\n","    console_handler = logging.StreamHandler()\n","    console_handler.setFormatter(log_format)\n","    logger.handlers = [console_handler]\n","    if log_file and log_file != '':\n","        file_handler = logging.FileHandler(log_file)\n","        file_handler.setLevel(log_file_level)\n","        # file_handler.setFormatter(log_format)\n","        logger.addHandler(file_handler)\n","    return logger\n","\n","class ProgressBar(object):\n","    # custom progress bar\n","    def __init__(self, n_total,width=30,desc = 'Training'):\n","        self.width = width\n","        self.n_total = n_total\n","        self.start_time = time.time()\n","        self.desc = desc\n","\n","    def __call__(self, step, info={}):\n","        now = time.time()\n","        current = step + 1\n","        recv_per = current / self.n_total\n","        bar = f'[{self.desc}] {current}/{self.n_total} ['\n","        if recv_per >= 1:\n","            recv_per = 1\n","        prog_width = int(self.width * recv_per)\n","        if prog_width > 0:\n","            bar += '=' * (prog_width - 1)\n","            if current< self.n_total:\n","                bar += \">\"\n","            else:\n","                bar += '='\n","        bar += '.' * (self.width - prog_width)\n","        bar += ']'\n","        show_bar = f\"\\r{bar}\"\n","        time_per_unit = (now - self.start_time) / current\n","        if current < self.n_total:\n","            eta = time_per_unit * (self.n_total - current)\n","            if eta > 3600:\n","                eta_format = ('%d:%02d:%02d' %\n","                              (eta // 3600, (eta % 3600) // 60, eta % 60))\n","            elif eta > 60:\n","                eta_format = '%d:%02d' % (eta // 60, eta % 60)\n","            else:\n","                eta_format = '%ds' % eta\n","            time_info = f' - ETA: {eta_format}'\n","        else:\n","            if time_per_unit >= 1:\n","                time_info = f' {time_per_unit:.1f}s/step'\n","            elif time_per_unit >= 1e-3:\n","                time_info = f' {time_per_unit * 1e3:.1f}ms/step'\n","            else:\n","                time_info = f' {time_per_unit * 1e6:.1f}us/step'\n","\n","        show_bar += time_info\n","        if len(info) != 0:\n","            show_info = f'{show_bar} ' + \\\n","                        \"-\".join([f' {key}: {value:.4f} ' if key != \"learning_rate\" else f' {key}: {value:.8f} ' for key, value in info.items()])\n","            print(show_info, end='')\n","        else:\n","            print(show_bar, end='')"]},{"cell_type":"markdown","id":"76f119da","metadata":{"papermill":{"duration":0.019108,"end_time":"2022-04-21T14:41:34.456571","exception":false,"start_time":"2022-04-21T14:41:34.437463","status":"completed"},"tags":[],"id":"76f119da"},"source":["<a id=\"section9\"><font color='#FF6F00'><h2>Custom Training Loop</h2></font></a>\n","\n","Here we define our Trainer class for training and evaluation loop. Custom training loops provide flexibility and a greater control on training. They also make it is easier to debug the model and the training loop. The following code block sets up these training steps:\n","\n","1. Iterate each epoch. An epoch is one pass through the dataset.\n","2. Within an epoch, iterate over each example in the training Dataset grabbing its features (x) and label (y).\n","3. Using the features, make a prediction and compare it with the label. Measure the inaccuracy of the prediction and use that to calculate the model's loss and gradients.\n","4. Use an optimizer to update the model's parameters.\n","5. Keep track of some stats for logging.\n","6. Repeat for each epoch.\n","\n","Below figure demonstrates the code flow of our `trainer.train()` method. One can read about Custom Training Loops [here](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough) and [here](https://www.tensorflow.org/tutorials/distribute/custom_training).\n","\n","\n","\n","\n","*Notes:*\n","- Iterate over the `train_dist_dataset` and `test_dist_dataset` using a for `x in ...` construct.\n","\n","- model, optimizer, and checkpoint must be created under `strategy.scope`.\n","\n","- `strategy.run()` replicates the provided computation and runs it with the distributed input. It returns results from each local replica in the strategy, and there are multiple ways to consume this result.\n","\n","- A model checkpointed with a `tf.distribute.Strategy` can be restored with or without a strategy.\n","\n","- `tf.function` helps to make graphs out of your programs. It is a transformation tool that creates Python-independent dataflow graphs out of your Python code. This will help you create performant and portable models, and it is required to use SavedModel. One can read more about `tf.function` [here](https://www.tensorflow.org/guide/function).\n","\n","- Using `tf.reduce_mean` is not recommended. Doing so divides the loss by actual per replica batch size which may vary step to step."]},{"cell_type":"code","execution_count":null,"id":"cff649b4","metadata":{"execution":{"iopub.execute_input":"2022-04-21T14:41:34.528720Z","iopub.status.busy":"2022-04-21T14:41:34.526882Z","iopub.status.idle":"2022-04-21T14:41:34.529394Z","shell.execute_reply":"2022-04-21T14:41:34.529860Z","shell.execute_reply.started":"2022-04-21T12:13:24.895982Z"},"papermill":{"duration":0.054201,"end_time":"2022-04-21T14:41:34.530014","exception":false,"start_time":"2022-04-21T14:41:34.475813","status":"completed"},"tags":[],"id":"cff649b4"},"outputs":[],"source":["class Trainer:\n","    def __init__(\n","        self, model, args, train_dataset, validation_dataset,\n","        num_train_examples, num_validation_examples\n","    ):\n","        self.model = model\n","        self.args = args\n","\n","        self.train_dataset = train_dataset\n","        self.num_train_examples = num_train_examples\n","\n","        self.validation_dataset = validation_dataset\n","        self.num_validation_examples = num_validation_examples\n","\n","        self.global_step = 0\n","        self.eval_loss = tf.keras.metrics.Sum()\n","\n","    def create_optimizer_and_scheduler(self, num_training_steps):\n","        # creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.\n","        num_warmup_steps = math.ceil(num_training_steps * self.args.warmup_ratio)\n","        self.optimizer, self.lr_scheduler = create_optimizer(\n","            init_lr=self.args.learning_rate,\n","            num_train_steps=num_training_steps,\n","            num_warmup_steps=num_warmup_steps,\n","            weight_decay_rate=self.args.weight_decay,\n","            adam_epsilon=self.args.adam_epsilon\n","        )\n","\n","    def evaluation_step(self, features, labels, nb_instances_in_global_batch):\n","        # forward pass\n","        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=False)[:2]\n","        loss, logits = outputs[:2]\n","        # loss scaling\n","        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n","        # add current batch loss\n","        self.eval_loss.update_state(scaled_loss)\n","\n","    @tf.function\n","    def distributed_evaluation_steps(self, batch):\n","        features = {k: v for k, v in batch.items() if 'labels' not in k}\n","        labels = batch['labels']\n","        nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n","        # strategy.run() expects args to be a list or tuple\n","        inputs = (features, labels, nb_instances)\n","        # `run` replicates the provided computation and runs with the distributed input\n","        strategy.run(self.evaluation_step, inputs)\n","\n","    def evaluate(self):\n","        # calculate total validation steps\n","        steps = math.ceil(self.num_validation_examples / self.args.validation_batch_size)\n","        # reset eval loss after every epoch\n","        self.eval_loss.reset_states()\n","        logs = {}\n","        pbar = ProgressBar(n_total=steps, desc='Evaluating')\n","        # iterate over validation dataset\n","        for step, batch in enumerate(self.validation_dataset):\n","            # distributed evaluation step\n","            self.distributed_evaluation_steps(batch)\n","            logs[\"eval_loss\"] = self.eval_loss.result() / (step + 1)\n","            pbar(step=step, info=logs)\n","            if step == steps - 1:\n","                break\n","        print(\"\\n------------- validation result -----------------\")\n","\n","    def apply_gradients(self, features, labels, nb_instances_in_global_batch):\n","        # forward pass\n","        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=True)[:2]\n","        loss, logits = outputs[:2]\n","        # loss scaling\n","        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n","        # calculate gradients\n","        gradients = tf.gradients(scaled_loss, self.model.trainable_variables)\n","        # convert gradients with nan value\n","        gradients = [g if g is not None else tf.zeros_like(v) for g, v in zip(gradients, self.model.trainable_variables)]\n","        # optimize the model\n","        self.optimizer.apply_gradients(list(zip(gradients, self.model.trainable_variables)))\n","        # add current batch loss\n","        self.train_loss.update_state(scaled_loss)\n","\n","    @tf.function\n","    def distributed_training_steps(self, batch):\n","        with strategy.scope():\n","            features = {k: v for k, v in batch.items() if 'labels' not in k}\n","            labels = batch['labels']\n","            nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n","            # strategy.run() expects args to be a list or tuple\n","            inputs = (features, labels, nb_instances)\n","            # `run` replicates the provided computation and runs with the distributed input.\n","            strategy.run(self.apply_gradients, inputs)\n","\n","    def train(self):\n","        # calculate total training steps\n","        num_updates_per_epoch = self.num_train_examples // args.train_batch_size\n","        self.steps_per_epoch = num_updates_per_epoch\n","        t_total = self.steps_per_epoch * self.args.epochs\n","\n","        with strategy.scope():\n","            # optimizer, and checkpoint must be created under `strategy.scope`\n","            # create optimizer and scheduler\n","            self.create_optimizer_and_scheduler(num_training_steps=t_total)\n","\n","            # create checkpoint manager\n","            folder = os.path.join(self.args.output_dir, self.args.checkpoint_dir)\n","            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)\n","            self.model.ckpt_manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n","            iterations = self.optimizer.iterations\n","\n","            logger.info(\"***** Running training *****\")\n","            logger.info(f\"  Num examples = {self.num_train_examples}\")\n","            logger.info(f\"  Num Epochs = {self.args.epochs}\")\n","            logger.info(f\"  Total train batch size (w. parallel & distributed) = {self.args.train_batch_size * n_replicas(strategy)}\")\n","            logger.info(f\"  Steps per epoch = {self.steps_per_epoch}\")\n","            logger.info(f\"  Total optimization steps = {t_total}\")\n","\n","            self.train_loss = tf.keras.metrics.Sum(name=\"training_loss\")\n","            start_time = datetime.datetime.now()\n","            for epoch_iter in range(self.args.epochs):\n","                # training loop\n","                logger.info(f\"Epoch {epoch_iter + 1}/{self.args.epochs}\")\n","\n","                pbar = ProgressBar(n_total=self.steps_per_epoch, desc='Training')\n","                # iterate over training dataset\n","                for step, batch in enumerate(self.train_dataset):\n","                    # distributed training step\n","                    self.distributed_training_steps(batch)\n","\n","                    self.global_step = iterations.numpy()\n","                    training_loss = self.train_loss.result() / (step + 1)\n","\n","                    logs = {}\n","                    logs[\"training_loss\"] = training_loss.numpy()\n","                    logs[\"learning_rate\"] = self.lr_scheduler(self.global_step).numpy()\n","                    pbar(step=step, info=logs)\n","\n","                    if self.global_step % self.steps_per_epoch == 0:\n","                        print(\"\\n------------- train result -----------------\")\n","                        # call to evaluation loop\n","                        self.evaluate()\n","                        # save checkpoint\n","                        ckpt_save_path = self.model.ckpt_manager.save()\n","                        logger.info(f\"Saving checkpoint at {ckpt_save_path}\")\n","                        break\n","\n","                # reset train loss after every epoch\n","                self.train_loss.reset_states()\n","            end_time = datetime.datetime.now()\n","            logger.info(f\"Training took: {str(end_time - start_time)}\")"]},{"cell_type":"markdown","id":"4725bcda","metadata":{"papermill":{"duration":0.019185,"end_time":"2022-04-21T14:41:34.569947","exception":false,"start_time":"2022-04-21T14:41:34.550762","status":"completed"},"tags":[],"id":"4725bcda"},"source":["<a id=\"section10\"><font color='#FF6F00'><h2>Run</h2></font></a>\n","\n","The `run()` function defines our execution process. We download, load and preprocess and convert our data into `tf.data.Dataset` format. We initialize tokenizer and model. The model needs to be created under `strategy.scope()`. We create instance of our `Trainer` and pass everything to `.train()` method for running our custom training loop. In the end we save our model and tokenizer using `.save_pretrained()` method."]},{"cell_type":"code","execution_count":null,"id":"5cbb3267","metadata":{"execution":{"iopub.execute_input":"2022-04-21T14:41:34.620423Z","iopub.status.busy":"2022-04-21T14:41:34.619584Z","iopub.status.idle":"2022-04-21T14:41:34.621558Z","shell.execute_reply":"2022-04-21T14:41:34.621967Z","shell.execute_reply.started":"2022-04-21T12:13:24.929780Z"},"papermill":{"duration":0.031726,"end_time":"2022-04-21T14:41:34.622121","exception":false,"start_time":"2022-04-21T14:41:34.590395","status":"completed"},"tags":[],"id":"5cbb3267"},"outputs":[],"source":["def run(args):\n","    logger.info(\" Starting training / evaluation\")\n","\n","    logger.info(\" Downloading Data Files\")\n","    dataset_path = download_dataset(args.cache_dir)\n","\n","    logger.info(\" Loading Data Files\")\n","    dataset = load_dataset('json', data_files=dataset_path)\n","    # train test split\n","    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n","\n","    logger.info(\" Initializing Tokenizer\")\n","    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n","\n","    logger.info(\" Preparing Features\")\n","    dataset = dataset.map(convert_examples_to_features, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"args\":args})\n","\n","    logger.info(\" Intializing training and validation dataset \")\n","    train_dataset = dataset['train']\n","    num_train_examples = len(dataset['train'])\n","    # create tf train dataset\n","    tf_train_dataset = get_train_tfdataset(train_dataset, num_train_examples, args)\n","\n","    validation_dataset = dataset['test']\n","    num_validation_examples = len(dataset['test'])\n","    # create tf validation dataset\n","    tf_validation_dataset = get_validation_tfdataset(train_dataset, num_validation_examples, args)\n","\n","    logger.info(f' Intializing model | {args.model_type.upper()} ')\n","    with strategy.scope():\n","        # model must be created under `strategy.scope`\n","        model = TFT5ForConditionalGeneration.from_pretrained(args.model_name_or_path, from_pt=True)\n","\n","    # custom training loop\n","    trainer = Trainer(model, args, tf_train_dataset, tf_validation_dataset, num_train_examples, num_validation_examples)\n","    trainer.train()\n","\n","    # save pretrained model and tokenizer\n","    logger.info(f\" Saving model in {args.save_dir}\")\n","    trainer.model.save_pretrained(args.save_dir)\n","    tokenizer.save_pretrained(args.save_dir)"]},{"cell_type":"markdown","id":"cad666c4","metadata":{"papermill":{"duration":0.020072,"end_time":"2022-04-21T14:41:34.661030","exception":false,"start_time":"2022-04-21T14:41:34.640958","status":"completed"},"tags":[],"id":"cad666c4"},"source":["<a id=\"section11\"><font color='#FF6F00'><h2>Execute</h2></font></a>\n","\n","Below we define our training arguments - model, data, optimizer, training and initialize directories. Initialize logger for logging and tracking metrics. We call `fix_all_seeds()` to set the global seed. Then finally we execute our `run()` method by passing our training `args`."]},{"cell_type":"code","execution_count":null,"id":"782e54c3","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-04-21T14:41:34.708529Z","iopub.status.busy":"2022-04-21T14:41:34.707631Z","iopub.status.idle":"2022-04-21T15:02:12.980082Z","shell.execute_reply":"2022-04-21T15:02:12.979615Z","shell.execute_reply.started":"2022-04-21T12:13:24.941619Z"},"papermill":{"duration":1238.300043,"end_time":"2022-04-21T15:02:12.980351","exception":false,"start_time":"2022-04-21T14:41:34.680308","status":"completed"},"tags":[],"colab":{"referenced_widgets":["10316615b6b6481a9aaf0783ebc9b627","cabd000858534783a4a66bd09cb7f5c4","bcb0ecd80c4845be8ba20ee56794a736","d395fe0992c543219f4713e95f9c810a","39012f3d58784b3282a1804e54535b4e","3441d035fddd43a6af03e3d7633a12f1","e8d3d7abc081477d9f3c81f2ee7cf703","95d6033643fb4be8a568763ec3d74df8","405e31075169426f8e9eb443f0ad96bb","706920a67bfc45f7b16e579ed858a37e","3ab3a699fc804ac3a6c261d4cfe3202d","b104159764dd4280b048052dfc01ffd2"]},"id":"782e54c3","outputId":"43226884-a76f-4bbf-923b-20f123d28955"},"outputs":[{"name":"stderr","output_type":"stream","text":["04/21/2022 14:41:34 - INFO - root -    Starting training / evaluation\n","04/21/2022 14:41:34 - INFO - root -    Downloading Data Files\n"]},{"name":"stdout","output_type":"stream","text":["Downloading data from https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl\n","565248/563743 [==============================] - 0s 0us/step\n","573440/563743 [==============================] - 0s 0us/step\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:41:35 - INFO - root -    Loading Data Files\n","04/21/2022 14:41:35 - WARNING - datasets.builder -   Using custom data configuration default-82fe3e58ac1966f8\n"]},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10316615b6b6481a9aaf0783ebc9b627","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cabd000858534783a4a66bd09cb7f5c4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bcb0ecd80c4845be8ba20ee56794a736","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:41:35 - INFO - root -    Initializing Tokenizer\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d395fe0992c543219f4713e95f9c810a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39012f3d58784b3282a1804e54535b4e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/687k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3441d035fddd43a6af03e3d7633a12f1","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/287k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8d3d7abc081477d9f3c81f2ee7cf703","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95d6033643fb4be8a568763ec3d74df8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:41:42 - INFO - root -    Preparing Features\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"405e31075169426f8e9eb443f0ad96bb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"706920a67bfc45f7b16e579ed858a37e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:41:45 - INFO - root -    Intializing training and validation dataset \n","04/21/2022 14:41:45 - INFO - root -    Intializing model | T5 \n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3ab3a699fc804ac3a6c261d4cfe3202d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b104159764dd4280b048052dfc01ffd2","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-04-21 14:42:17.180076: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n","04/21/2022 14:42:21 - INFO - root -   ***** Running training *****\n","04/21/2022 14:42:21 - INFO - root -     Num examples = 876\n","04/21/2022 14:42:21 - INFO - root -     Num Epochs = 20\n","04/21/2022 14:42:21 - INFO - root -     Total train batch size (w. parallel & distributed) = 8\n","04/21/2022 14:42:21 - INFO - root -     Steps per epoch = 109\n","04/21/2022 14:42:21 - INFO - root -     Total optimization steps = 2180\n","04/21/2022 14:42:21 - INFO - root -   Epoch 1/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 3.1s/step  training_loss: 3.6843 - learning_rate: 0.00007500 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 914.7ms/step  eval_loss: 1.5429 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:48:27 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-1\n","04/21/2022 14:48:27 - INFO - root -   Epoch 2/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 255.7ms/step  training_loss: 1.5106 - learning_rate: 0.00015000 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 5.0s/step  eval_loss: 1.0644 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:50:10 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-2\n","04/21/2022 14:50:11 - INFO - root -   Epoch 3/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 258.0ms/step  training_loss: 1.2020 - learning_rate: 0.00022500 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.3ms/step  eval_loss: 0.8043 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:50:49 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-3\n","04/21/2022 14:50:50 - INFO - root -   Epoch 4/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 259.2ms/step  training_loss: 0.9767 - learning_rate: 0.00030000 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 76.2ms/step  eval_loss: 0.6287 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:51:28 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-4\n","04/21/2022 14:51:28 - INFO - root -   Epoch 5/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 258.6ms/step  training_loss: 0.8047 - learning_rate: 0.00028125 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.7ms/step  eval_loss: 0.4416 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:52:07 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-5\n","04/21/2022 14:52:08 - INFO - root -   Epoch 6/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 259.7ms/step  training_loss: 0.6212 - learning_rate: 0.00026250 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 79.4ms/step  eval_loss: 0.3410 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:52:47 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-6\n","04/21/2022 14:52:47 - INFO - root -   Epoch 7/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 258.2ms/step  training_loss: 0.4821 - learning_rate: 0.00024375 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.2ms/step  eval_loss: 0.2421 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:53:26 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-7\n","04/21/2022 14:53:28 - INFO - root -   Epoch 8/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 259.3ms/step  training_loss: 0.3874 - learning_rate: 0.00022500 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.4ms/step  eval_loss: 0.1564 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:54:07 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-8\n","04/21/2022 14:54:07 - INFO - root -   Epoch 9/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 258.8ms/step  training_loss: 0.2998 - learning_rate: 0.00020625 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 75.2ms/step  eval_loss: 0.1062 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:54:46 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-9\n","04/21/2022 14:54:46 - INFO - root -   Epoch 10/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 259.1ms/step  training_loss: 0.2476 - learning_rate: 0.00018750 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.3ms/step  eval_loss: 0.0675 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:55:25 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-10\n","04/21/2022 14:55:25 - INFO - root -   Epoch 11/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 258.3ms/step  training_loss: 0.1836 - learning_rate: 0.00016875 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.3ms/step  eval_loss: 0.0429 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:56:04 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-11\n","04/21/2022 14:56:06 - INFO - root -   Epoch 12/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 257.0ms/step  training_loss: 0.1393 - learning_rate: 0.00015000 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 83.5ms/step  eval_loss: 0.0317 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:56:45 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-12\n","04/21/2022 14:56:47 - INFO - root -   Epoch 13/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 257.0ms/step  training_loss: 0.1142 - learning_rate: 0.00013125 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 74.1ms/step  eval_loss: 0.0257 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:57:26 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-13\n","04/21/2022 14:57:28 - INFO - root -   Epoch 14/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 258.0ms/step  training_loss: 0.0865 - learning_rate: 0.00011250 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.4ms/step  eval_loss: 0.0139 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:58:07 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-14\n","04/21/2022 14:58:07 - INFO - root -   Epoch 15/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 257.8ms/step  training_loss: 0.0706 - learning_rate: 0.00009375 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 90.5ms/step  eval_loss: 0.0101 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:58:47 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-15\n","04/21/2022 14:58:48 - INFO - root -   Epoch 16/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 257.0ms/step  training_loss: 0.0522 - learning_rate: 0.00007500 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.6ms/step  eval_loss: 0.0078 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 14:59:28 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-16\n","04/21/2022 14:59:29 - INFO - root -   Epoch 17/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 259.8ms/step  training_loss: 0.0445 - learning_rate: 0.00005625 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 79.1ms/step  eval_loss: 0.0041 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:00:08 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-17\n","04/21/2022 15:00:08 - INFO - root -   Epoch 18/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 257.2ms/step  training_loss: 0.0367 - learning_rate: 0.00003750 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 79.2ms/step  eval_loss: 0.0029 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:00:47 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-18\n","04/21/2022 15:00:47 - INFO - root -   Epoch 19/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 260.0ms/step  training_loss: 0.0313 - learning_rate: 0.00001875 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 77.7ms/step  eval_loss: 0.0024 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:01:26 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-19\n","04/21/2022 15:01:28 - INFO - root -   Epoch 20/20\n"]},{"name":"stdout","output_type":"stream","text":["[Training] 109/109 [==============================] 256.8ms/step  training_loss: 0.0290 - learning_rate: 0.00000000 \n","------------- train result -----------------\n","[Evaluating] 13/13 [==============================] 78.2ms/step  eval_loss: 0.0024 \n","------------- validation result -----------------\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:07 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-20\n","04/21/2022 15:02:07 - INFO - root -   Training took: 0:19:46.308197\n","04/21/2022 15:02:07 - INFO - root -    Saving model in runs//saved_model/\n"]}],"source":["class Args:\n","    # define training arguments\n","\n","    # MODEL\n","    model_type = 't5'\n","    tokenizer_name = 'Salesforce/codet5-base'\n","    model_name_or_path = 'Salesforce/codet5-base'\n","\n","    # DATA\n","    train_batch_size = 8\n","    validation_batch_size = 8\n","    max_input_length = 48\n","    max_target_length = 128\n","    prefix = \"Generate Python: \"\n","\n","    # OPTIMIZER\n","    learning_rate = 3e-4\n","    weight_decay = 1e-4\n","    warmup_ratio = 0.2\n","    adam_epsilon = 1e-8\n","\n","    # TRAINING\n","    seed = 2022\n","    epochs = 20\n","\n","    # DIRECTORIES\n","    output_dir = \"runs/\"\n","    logging_dir = f\"{output_dir}/logs/\"\n","    checkpoint_dir = f\"checkpoint\"\n","    save_dir = f\"{output_dir}/saved_model/\"\n","    cache_dir = '../working/'\n","    Path(output_dir).mkdir(parents=True, exist_ok=True)\n","    Path(logging_dir).mkdir(parents=True, exist_ok=True)\n","    Path(save_dir).mkdir(parents=True, exist_ok=True)\n","\n","\n","# initialize training arguments\n","args = Args()\n","# initialize logger\n","logger = init_logger(log_file=os.path.join(args.logging_dir, f\"{args.model_type}-{time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())}.log\"))\n","# fix all seeds\n","fix_all_seeds(args.seed)\n","\n","if __name__ == \"__main__\":\n","    # run training and evaluation\n","    dataset = run(args)"]},{"cell_type":"markdown","id":"cf0a2ea3","metadata":{"papermill":{"duration":0.757657,"end_time":"2022-04-21T15:02:14.484221","exception":false,"start_time":"2022-04-21T15:02:13.726564","status":"completed"},"tags":[],"id":"cf0a2ea3"},"source":["<a id=\"section12\"><font color='#FF6F00'><h2>Predict</h2></font></a>\n","\n","- The `predict_from_dataset()` and `predict_from_text()` method is where we do our predictions/inference. Here we are using the texts in test set to generate our code in `predict_from_dataset()` but one can call `predict_from_text()` and provide custom input.\n","\n","- `run_predict()` is the main function which calls `model.generate()` method to output the model predictions using decoding technique.\n","\n","- In `predict_from_dataset()` we randomly choose an index from the test dataset, so everytime the function is called we randomly sample an index from the dataset.\n","\n","- Notes:\n","    -  Here we are using Top-p (nucleus) sampling technique for decoding. In simple, top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Here we can use Top-p sampling by setting 0 < `top_p` < 1.\n","\n","    - `Top-p` can also be used in combination with `Top-K`, which can avoid very low ranked words while allowing for some dynamic selection.\n","\n","    - `Reptition_penalty` can be used to penalize words that were already generated or belong to the context.\n","\n","    - To get multiple independently sampled outputs, we can again set the parameter `num_return_sequences` > 1."]},{"cell_type":"code","execution_count":null,"id":"7b198e92","metadata":{"execution":{"iopub.execute_input":"2022-04-21T15:02:16.317528Z","iopub.status.busy":"2022-04-21T15:02:16.315854Z","iopub.status.idle":"2022-04-21T15:02:16.318135Z","shell.execute_reply":"2022-04-21T15:02:16.318553Z","shell.execute_reply.started":"2022-04-21T12:34:12.767944Z"},"papermill":{"duration":1.039229,"end_time":"2022-04-21T15:02:16.318697","exception":false,"start_time":"2022-04-21T15:02:15.279468","status":"completed"},"tags":[],"id":"7b198e92"},"outputs":[],"source":["def run_predict(args, text):\n","    # load saved finetuned model\n","    model = TFT5ForConditionalGeneration.from_pretrained(args.save_dir)\n","    # load saved tokenizer\n","    tokenizer = RobertaTokenizer.from_pretrained(args.save_dir)\n","\n","     # encode texts by prepending the task for input sequence and appending the test sequence\n","    query = args.prefix + text\n","    encoded_text = tokenizer(query, return_tensors='tf', padding='max_length', truncation=True, max_length=args.max_input_length)\n","\n","    # inference\n","    generated_code = model.generate(\n","        encoded_text[\"input_ids\"], attention_mask=encoded_text[\"attention_mask\"],\n","        max_length=args.max_target_length, top_p=0.95, top_k=50, repetition_penalty=2, num_return_sequences=1\n","    )\n","\n","    # decode generated tokens\n","    decoded_code = tokenizer.decode(generated_code.numpy()[0], skip_special_tokens=True)\n","    return decoded_code\n","\n","def predict_from_dataset(args):\n","    # load using hf datasets\n","    dataset = load_dataset('json', data_files='../working/mbpp.jsonl')\n","    # train test split\n","    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n","    test_dataset = dataset['test']\n","\n","    # randomly select an index from the validation dataset\n","    index = random.randint(0, len(test_dataset))\n","    text = test_dataset[index]['text']\n","    code = test_dataset[index]['code']\n","\n","    # run-predict on text\n","    decoded_code = run_predict(args, text)\n","\n","    print(\"#\" * 25); print(\"QUERY: \", text);\n","    print()\n","    print('#' * 25); print(\"ORIGINAL: \"); print(\"\\n\", code);\n","    print()\n","    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);\n","\n","def predict_from_text(args, text):\n","    # run-predict on text\n","    decoded_code = run_predict(args, text)\n","    print(\"#\" * 25); print(\"QUERY: \", text);\n","    print()\n","    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);"]},{"cell_type":"markdown","id":"c03869c0","metadata":{"papermill":{"duration":0.80672,"end_time":"2022-04-21T15:02:17.898195","exception":false,"start_time":"2022-04-21T15:02:17.091475","status":"completed"},"tags":[],"id":"c03869c0"},"source":["<a id=\"section12a\"><font color='#FF6F00'><h3>Predict from Dataset</h3></font></a>"]},{"cell_type":"code","execution_count":null,"id":"73a18614","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-04-21T15:02:19.463457Z","iopub.status.busy":"2022-04-21T15:02:19.462816Z","iopub.status.idle":"2022-04-21T15:02:41.941221Z","shell.execute_reply":"2022-04-21T15:02:41.940701Z","shell.execute_reply.started":"2022-04-21T12:34:12.795910Z"},"papermill":{"duration":23.235842,"end_time":"2022-04-21T15:02:41.941361","exception":false,"start_time":"2022-04-21T15:02:18.705519","status":"completed"},"tags":[],"colab":{"referenced_widgets":["745b026719d248009282110cae1af4f6","a11f9c0023fe44e78c8a340a0dcd0e1c","7283e221f5564b7681226532da00e280"]},"id":"73a18614","outputId":"064b96b9-85ef-4170-b8c4-0b3f569de05e"},"outputs":[{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:20 - WARNING - datasets.builder -   Using custom data configuration default-82fe3e58ac1966f8\n","04/21/2022 15:02:20 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"745b026719d248009282110cae1af4f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:20 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-64c56dfca2d42905.arrow and /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-84194af6b1765ad6.arrow\n"]},{"name":"stdout","output_type":"stream","text":["#########################\n","QUERY:  Write a function to convert the given tuples into set.\n","\n","#########################\n","ORIGINAL: \n","\n"," def tuple_to_set(t):\r\n","  s = set(t)\r\n","  return (s) \n","\n","#########################\n","GENERATED: \n","\n"," def tuple_set(test_tup):\r\n","  res = set([tuple() for ele in test_ t up]) \r\n","  return (res)\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:26 - WARNING - datasets.builder -   Using custom data configuration default-82fe3e58ac1966f8\n","04/21/2022 15:02:26 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a11f9c0023fe44e78c8a340a0dcd0e1c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:26 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-64c56dfca2d42905.arrow and /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-84194af6b1765ad6.arrow\n"]},{"name":"stdout","output_type":"stream","text":["#########################\n","QUERY:  Write a function to check for a number at the end of a string.\n","\n","#########################\n","ORIGINAL: \n","\n"," import re\r\n","def end_num(string):\r\n","    text = re.compile(r\".*[0-9]$\")\r\n","    if text.match(string):\r\n","        return True\r\n","    else:\r\n","        return False\n","\n","#########################\n","GENERATED: \n","\n"," def check_number(text):\r\n","  if re.search(\"[0-9]\", text) : \r\n","    return (\"Valid\")   else:    \r\n","      return \"Invalid\"\n"]},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:35 - WARNING - datasets.builder -   Using custom data configuration default-82fe3e58ac1966f8\n","04/21/2022 15:02:35 - WARNING - datasets.builder -   Reusing dataset json (/root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7283e221f5564b7681226532da00e280","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["04/21/2022 15:02:35 - WARNING - datasets.arrow_dataset -   Loading cached split indices for dataset at /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-64c56dfca2d42905.arrow and /root/.cache/huggingface/datasets/json/default-82fe3e58ac1966f8/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-84194af6b1765ad6.arrow\n"]},{"name":"stdout","output_type":"stream","text":["#########################\n","QUERY:  Write a function to convert camel case string to snake case string by using regex.\n","\n","#########################\n","ORIGINAL: \n","\n"," import re\r\n","def camel_to_snake(text):\r\n","  str1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', text)\r\n","  return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', str1).lower()\n","\n","#########################\n","GENERATED: \n","\n"," import re\r\n","def snake_to_camel(word):\r\n","  return ''.join([x for x in word if len('_') == 1])\n"]}],"source":["# example 1\n","predict_from_dataset(args)\n","# example 2\n","predict_from_dataset(args)\n","# example 3\n","predict_from_dataset(args)"]},{"cell_type":"markdown","id":"6f94c82b","metadata":{"papermill":{"duration":0.750688,"end_time":"2022-04-21T15:02:43.468794","exception":false,"start_time":"2022-04-21T15:02:42.718106","status":"completed"},"tags":[],"id":"6f94c82b"},"source":["<a id=\"section12b\"><font color='#FF6F00'><h3>Predict from Text</h3></font></a>"]},{"cell_type":"code","execution_count":null,"id":"d94370e4","metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2022-04-21T15:02:44.981575Z","iopub.status.busy":"2022-04-21T15:02:44.980741Z","iopub.status.idle":"2022-04-21T15:03:23.498240Z","shell.execute_reply":"2022-04-21T15:03:23.498842Z","shell.execute_reply.started":"2022-04-21T12:34:43.909881Z"},"papermill":{"duration":39.27622,"end_time":"2022-04-21T15:03:23.499021","exception":false,"start_time":"2022-04-21T15:02:44.222801","status":"completed"},"tags":[],"id":"d94370e4","outputId":"2b5869f4-4424-4293-dfee-bda12f834ac0"},"outputs":[{"name":"stdout","output_type":"stream","text":["#########################\n","QUERY:  Write a function to add two random numbers\n","\n","#########################\n","GENERATED: \n","\n"," def add_random(a,b):\r\n","    if a + b > c:\r\n","        return None\r\n","     else::\r\n","         random = [0 for i in range (min()], max((gcd(), num)) % d])] \r\n","      result1=add(_[i]|\"\")\r\n","  while len($result2), 1 >=len('x') and count < 2]:\r\n","   temp.append([j]))\r\n","                sums += yield from hash sdict\n","\n","#########################\n","QUERY:  Write a function to find the frequency of items in a list\n","\n","#########################\n","GENERATED: \n","\n"," import collections\r\n","def freq_count(list1): \r\n","    dict = defaultdict()   for i in list 1: \r\n","        keys=dict.keys():    \r\n","            value[i] += one\r\n","    dic_data = {}\r\n","    if key notin dictionary._values]:      return {key :value}\r\n","         else::################################        , values=[dictionary[:-one]]]) \n","\n","#########################\n","QUERY:  Write a function to concatenate two dictionary\n","\n","#########################\n","GENERATED: \n","\n"," def concatenate_dict(d1, d2):\r\n","    result = {k: v for k, val in dict.items() if not n % len([key] + value) == 0])} \r\n","    return list (result).values())\n","\n"]}],"source":["# example 1\n","predict_from_text(args, \"Write a function to add two random numbers\"); print()\n","# example 2\n","predict_from_text(args, \"Write a function to find the frequency of items in a list\"); print()\n","# example 3\n","predict_from_text(args, \"Write a function to concatenate two dictionary\"); print()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":1349.363152,"end_time":"2022-04-21T15:03:29.260899","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-04-21T14:40:59.897747","version":"2.3.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}